# 神经网络的模型压缩和硬件加速：综述

## 摘要

由于摩尔定律的可预见的终结，在通用处理器的改进速度下降的背景下，特定领域的硬件正成为一个有前途的话题。机器学习，尤其是深度神经网络（DNN），因为其在各种人工智能（AI）任务中的成功应用，已成为了最受瞩目的领域。  DNN的无与伦比的准确性是通过消耗大量的内存以及较高的计算复杂性来实现的，这极大地阻碍了它们在嵌入式系统中的部署。 因此，自然而然地提出了DNN压缩概念，并将其广泛用于节省内存和计算加速。 在过去的几年中，涌现了大量压缩技术，以在计算效率和应用准确性之间寻求令人满意的折衷。最近，这一浪潮已经蔓延到神经网络加速器的设计中，以获取超高的性能。 但是，相关工作量巨大以及报告的方法差异很大。这项混乱的研究促使我们对在高效压缩和执行DNN而不损害准确性的目标方面的最新进展进行全面的综述，涉及高级算法及其在硬件设计中的应用。 在本文中，我们回顾了主流的压缩方法，例如紧凑模型，张量分解，数据量化和网络稀疏化。 我们解释了它们的压缩原理，评估指标，敏感性分析和联合使用方法。 然后，我们回答了如何在神经网络加速器的设计中利用这些方法的问题，并提出了最新的硬件架构。 最后，我们讨论了几个问题，例如公平比较，测试工作负载，自动压缩，对安全性的影响以及框架/硬件级别的支持，并给出了该领域中研究方向以及可能遇到的挑战。 本文试图使读者快速建立起神经网络压缩和加速的概况，清楚地评估各种方法，并自信地以正确的方式开始。  

##  1. 简介，动机与概述

略

##  2. 神经网络概论

略

##  3. 神经网络压缩：算法

在本节中，我们首先介绍本文针对各种神经网络压缩方法的总体分类，然后逐一详细介绍特定的方法。

### A. 压缩方法分类法

图3展示了我们对神经网络压缩中的不同方法进行了分类的分类法。
![Fig_3](Model Compression and Hardware Acceleration for Neural Networks A Comprehensive Survey.assets\Fig_3.png)
注意，这里为了清晰起见，该图中未包括采用多种方法进行的联合压缩。

## 4. 神经网络压缩：硬件

略

## 5. 总结与讨论

略
